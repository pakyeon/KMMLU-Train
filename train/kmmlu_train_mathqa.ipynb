{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 GB of memory reserved.\n",
      "GPU = NVIDIA GeForce RTX 3060 Laptop GPU. Max memory = 6.0 GB.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# í˜„ì¬ ì‚¬ìš© ì¤‘ì¸ GPUì˜ ì†ì„± ì •ë³´ ê°€ì ¸ì˜¤ê¸°\n",
    "gpu_stats = torch.cuda.get_device_properties(0)  # ì²« ë²ˆì§¸ GPU ì†ì„± ì •ë³´ë¥¼ ê°€ì ¸ì˜´\n",
    "\n",
    "# í˜„ì¬ GPUì—ì„œ ì˜ˆì•½ëœ ë©”ëª¨ë¦¬ ì–‘ì„ GB ë‹¨ìœ„ë¡œ ê³„ì‚°í•˜ì—¬ ì¶œë ¥\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "print(f\"{start_gpu_memory} GB of memory reserved.\")  # ì˜ˆì•½ëœ ë©”ëª¨ë¦¬ ì–‘ ì¶œë ¥\n",
    "\n",
    "# GPUì˜ ì „ì²´ ë©”ëª¨ë¦¬ í¬ê¸°ë¥¼ GB ë‹¨ìœ„ë¡œ ê³„ì‚°í•˜ì—¬ ì¶œë ¥\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")  # GPU ì´ë¦„ê³¼ ìµœëŒ€ ë©”ëª¨ë¦¬ ì¶œë ¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "==((====))==  Unsloth 2024.10.7: Fast Llama patching. Transformers = 4.46.1.\n",
      "   \\\\   /|    GPU: NVIDIA GeForce RTX 3060 Laptop GPU. Max memory: 6.0 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.5.1+cu124. CUDA = 8.6. CUDA Toolkit = 12.4.\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.28.post3. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b84152fc3bc414b9666ac4c06c23c4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Bllossom/llama-3.2-Korean-Bllossom-3B does not have a padding token! Will use pad_token = <|finetune_right_pad_id|>.\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "from peft import PeftModel\n",
    "\n",
    "# ëª¨ë¸ì˜ ë°ì´í„° íƒ€ì…ì„ ì„¤ì •\n",
    "dtype = torch.bfloat16\n",
    "\n",
    "# ëª¨ë¸ì„ 4ë¹„íŠ¸ ì–‘ìí™”í•˜ì—¬ ë¡œë“œí• ì§€ ì—¬ë¶€\n",
    "load_in_4bit = True\n",
    "\n",
    "# ë°°ì¹˜ í¬ê¸° ì„¤ì •\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "# ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ì´ˆê¸°í™”\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=\"Bllossom/llama-3.2-Korean-Bllossom-3B\",\n",
    "    max_seq_length=200,\n",
    "    dtype=dtype,\n",
    "    load_in_4bit=load_in_4bit,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "tokenizer.padding_side = \"right\"\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"allenai/math_qa\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def preprocess_options(dataset):\n",
    "    \n",
    "    processed_options = []\n",
    "        \n",
    "    for options_string in dataset[\"options\"]:\n",
    "        options = [\n",
    "            re.sub(r'^[a-e] \\) ', '', option.strip()).upper()\n",
    "            for option in re.findall(r'[a-e] \\) [^,]+', options_string)\n",
    "        ]\n",
    "        processed_options.append(options)\n",
    "        \n",
    "    return {\n",
    "        \"options\" : processed_options\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = dataset['train']\n",
    "eval_data = dataset['validation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_data.map(preprocess_options, batched=True, num_proc=4,)\n",
    "eval_dataset = eval_data.map(preprocess_options, batched=True, num_proc=4,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Problem': \"the banker ' s gain of a certain sum due 3 years hence at 10 % per annum is rs . 36 . what is the present worth ?\",\n",
       " 'Rationale': '\"explanation : t = 3 years r = 10 % td = ( bg Ã— 100 ) / tr = ( 36 Ã— 100 ) / ( 3 Ã— 10 ) = 12 Ã— 10 = rs . 120 td = ( pw Ã— tr ) / 100 â‡’ 120 = ( pw Ã— 3 Ã— 10 ) / 100 â‡’ 1200 = pw Ã— 3 pw = 1200 / 3 = rs . 400 answer : option a\"',\n",
       " 'options': ['RS . 400', 'RS . 300', 'RS . 500', 'RS . 350', 'NONE OF THESE'],\n",
       " 'correct': 'a',\n",
       " 'annotated_formula': 'divide(multiply(const_100, divide(multiply(36, const_100), multiply(3, 10))), multiply(3, 10))',\n",
       " 'linear_formula': 'multiply(n2,const_100)|multiply(n0,n1)|divide(#0,#1)|multiply(#2,const_100)|divide(#3,#1)|',\n",
       " 'category': 'gain'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "ë¬¸ì œ: {question}\n",
    "A. {A}\n",
    "B. {B}\n",
    "C. {C}\n",
    "D. {D}\n",
    "ì •ë‹µ:{answer}\"\"\"\n",
    "\n",
    "EOS_TOKEN = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(dataset):\n",
    "    question = dataset['Problem']\n",
    "    options_list = dataset['options']\n",
    "    answers = dataset['correct']\n",
    "\n",
    "    result = []\n",
    "    \n",
    "    for q, option, ans in zip(question, options_list, answers):\n",
    "        A = option[0]\n",
    "        B = option[1]\n",
    "        C = option[2]\n",
    "        D = option[3]\n",
    "        \n",
    "        ans = ans.upper()\n",
    "        \n",
    "        text = prompt_template.format(question=q, A=A, B=B, C=C, D=D, answer=ans)\n",
    "        text += EOS_TOKEN\n",
    "        result.append(text)\n",
    "        \n",
    "    return {\n",
    "        \"text\" : result\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e69f0f4834d428f902f107f43b6a88f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/29837 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de03e2b0e929429fbd9b8d413d28ed0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/4475 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# dataset ì— preprocess_function ì ìš©\n",
    "train_dataset = train_dataset.map(preprocess_function, batched=True, num_proc=4, remove_columns=train_dataset.column_names)\n",
    "eval_dataset = eval_dataset.map(preprocess_function, batched=True, num_proc=4, remove_columns=eval_dataset.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ë¬¸ì œ: the banker ' s gain of a certain sum due 3 years hence at 10 % per annum is rs . 36 . what is the present worth ?\n",
      "A. RS . 400\n",
      "B. RS . 300\n",
      "C. RS . 500\n",
      "D. RS . 350\n",
      "ì •ë‹µ:A<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset['text'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ëª¨ë¸ ì´ˆê¸°í™”\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=8,  # 0ë³´ë‹¤ í° ì–´ë–¤ ìˆ«ìë„ ì„ íƒ ê°€ëŠ¥! 8, 16, 32, 64, 128ì´ ê¶Œì¥ë©ë‹ˆë‹¤.\n",
    "    lora_alpha=16,  # LoRA ì•ŒíŒŒ ê°’ì„ ì„¤ì •í•©ë‹ˆë‹¤. # íŠœí† ë¦¬ì–¼ì€ 16\n",
    "    lora_dropout=0.01,  # ë“œë¡­ì•„ì›ƒì„ ì§€ì›í•©ë‹ˆë‹¤. # # Supports any, but = 0 is optimized\n",
    "    target_modules=[\n",
    "        \"q_proj\", # query\n",
    "        \"k_proj\", # key\n",
    "        \"v_proj\", # value\n",
    "        \"o_proj\", # output (ì–´í…ì…˜ ìµœì¢… ì¶œë ¥)\n",
    "        \"gate_proj\", # ê²Œì´íŠ¸ ì¡°ì ˆ ì‹œ ì‚¬ìš©\n",
    "        \"up_proj\", # ì°¨ì› í™•ì¥ ì‹œ ì‚¬ìš©\n",
    "        \"down_proj\", # ì°¨ì› ì¶•ì†Œ ì‹œ ì‚¬ìš©\n",
    "    ],  # íƒ€ê²Ÿ ëª¨ë“ˆì„ ì§€ì •í•©ë‹ˆë‹¤.\n",
    "    bias=\"none\",  # ë°”ì´ì–´ìŠ¤ë¥¼ ì§€ì›í•©ë‹ˆë‹¤.\n",
    "    # True ë˜ëŠ” \"unsloth\"ë¥¼ ì‚¬ìš©í•˜ì—¬ ë§¤ìš° ê¸´ ì»¨í…ìŠ¤íŠ¸ì— ëŒ€í•´ VRAMì„ 30% ëœ ì‚¬ìš©í•˜ê³ , 2ë°° ë” í° ë°°ì¹˜ í¬ê¸°ë¥¼ ì§€ì›í•©ë‹ˆë‹¤.\n",
    "    use_gradient_checkpointing=\"unsloth\",\n",
    "    random_state=3407,  # ë‚œìˆ˜ ìƒíƒœë¥¼ ì„¤ì •í•©ë‹ˆë‹¤. # ê³µì‹ : 3407\n",
    "    use_rslora=True,  # ìˆœìœ„ ì•ˆì •í™” LoRAë¥¼ ì§€ì›í•©ë‹ˆë‹¤.\n",
    "    loftq_config=None,  # LoftQë¥¼ ì§€ì›í•©ë‹ˆë‹¤.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "# ì´ ìŠ¤í… ìˆ˜ ê³„ì‚°\n",
    "total_steps = len(train_dataset) // (4 * 4)  # batch_size * gradient_accumulation_steps\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    # ë°°ì¹˜ ì„¤ì •\n",
    "    per_device_train_batch_size=4,       # í›ˆë ¨ ì‹œ ë°°ì¹˜ ì‚¬ì´ì¦ˆ\n",
    "    per_device_eval_batch_size=4,        # í‰ê°€ ì‹œ ë°°ì¹˜ ì‚¬ì´ì¦ˆ\n",
    "    gradient_accumulation_steps=4,       # gradient ëˆ„ì  ë‹¨ê³„ ìˆ˜ (ë©”ëª¨ë¦¬ ìµœì í™”)\n",
    "\n",
    "    # ì›œì—… ë° ì—í­\n",
    "    warmup_ratio=0.1,                    # ì „ì²´ ìŠ¤í… ëŒ€ë¹„ warmup ë¹„ìœ¨\n",
    "    num_train_epochs=1,                  # ì´ í•™ìŠµ ì—í­ ìˆ˜\n",
    "\n",
    "    # ìµœì í™” ë° í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ë§\n",
    "    learning_rate=2e-4,                  # ì´ˆê¸° í•™ìŠµë¥ \n",
    "    optim=\"adamw_bnb_8bit\",              # 8ë¹„íŠ¸ AdamW ìµœì í™”\n",
    "    weight_decay=0.01,                   # ê°€ì¤‘ì¹˜ ê°ì†Œìœ¨\n",
    "    lr_scheduler_type=\"cosine\",          # ì½”ì‚¬ì¸ í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ëŸ¬\n",
    "    # lr_scheduler_type=\"cosine_with_restarts\",  # ì½”ì‚¬ì¸ ìŠ¤ì¼€ì¤„ëŸ¬ì— ì¬ì‹œì‘ ì¶”ê°€ ê°€ëŠ¥\n",
    "\n",
    "    # í•˜ë“œì›¨ì–´ í˜¸í™˜ì´ í•„ìš”í•œ ìµœì í™”\n",
    "    fp16=not torch.cuda.is_bf16_supported(),  # FP16 ì‚¬ìš© (BF16 ë¯¸ì§€ì› ì‹œ)\n",
    "    bf16=torch.cuda.is_bf16_supported(),      # BF16 ì‚¬ìš© (ì§€ì› ì‹œ)\n",
    "    torch_compile=True,                       # PyTorch ì»´íŒŒì¼ì„ í†µí•œ ë©”ëª¨ë¦¬, í•™ìŠµ ìµœì í™”\n",
    "\n",
    "    # ë¡œê¹… ë° wandb\n",
    "    logging_steps=total_steps // 20,       # ì „ì²´ ìŠ¤í…ì˜ 5%ë§ˆë‹¤ ë¡œê¹…\n",
    "    report_to=\"wandb\",                    # wandb\n",
    "    run_name=\"KMMLU_ALL_CATEGORIES\",      # run ì´ë¦„\n",
    "\n",
    "    # í‰ê°€ ë° ì²´í¬í¬ì¸íŠ¸\n",
    "    eval_strategy=\"steps\",                # ì •ê¸°ì ì¸ í‰ê°€\n",
    "    eval_steps=total_steps // 20,         # ì „ì²´ ìŠ¤í…ì˜ 5%ë§ˆë‹¤ í‰ê°€\n",
    "    load_best_model_at_end=True,          # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì €ì¥\n",
    "    metric_for_best_model=\"eval_loss\",    # ì„±ëŠ¥ í‰ê°€ ê¸°ì¤€ (loss)\n",
    "    greater_is_better=False,              # ë‚®ì€ lossê°€ ë” ì¢‹ì€ ëª¨ë¸ë¡œ í‰ê°€\n",
    "\n",
    "    # ì²´í¬í¬ì¸íŠ¸ ì €ì¥\n",
    "    save_strategy=\"steps\",                # ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ì „ëµ (ìŠ¤í… ë‹¨ìœ„)\n",
    "    save_steps=total_steps // 20,         # ì „ì²´ ìŠ¤í…ì˜ 5%ë§ˆë‹¤ ì €ì¥\n",
    "    save_total_limit=5,                   # ìµœëŒ€ ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ê°œìˆ˜\n",
    "\n",
    "    # ê¸°íƒ€ ì„¤ì •\n",
    "    seed=3407,                            # ì‹œë“œ ê³ ì •\n",
    "    output_dir=\"outputs\",                 # ì¶œë ¥ ê²½ë¡œ\n",
    "    max_grad_norm=1.0,                    # ìµœëŒ€ gradient norm ì„¤ì •\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    dataset_text_field=\"text\",\n",
    "    dataset_num_proc=4,\n",
    "    packing=False,\n",
    "    args=training_args,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "wandb.login()\n",
    "\n",
    "trainer_stats = trainer.train()\n",
    "\n",
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kmmlu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
