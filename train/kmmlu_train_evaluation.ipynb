{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# 현재 사용 중인 GPU의 속성 정보 가져오기\n",
    "gpu_stats = torch.cuda.get_device_properties(0)  # 첫 번째 GPU 속성 정보를 가져옴\n",
    "\n",
    "# 현재 GPU에서 예약된 메모리 양을 GB 단위로 계산하여 출력\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "print(f\"{start_gpu_memory} GB of memory reserved.\")  # 예약된 메모리 양 출력\n",
    "\n",
    "# GPU의 전체 메모리 크기를 GB 단위로 계산하여 출력\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")  # GPU 이름과 최대 메모리 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gc\n",
    "\n",
    "# GPU 메모리 초기화\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "# 모델의 데이터 타입을 설정 (bfloat16: 16비트 정밀도를 사용하여 메모리 사용량 절감)\n",
    "dtype = torch.bfloat16\n",
    "\n",
    "# 모델을 4비트 양자화하여 로드할지 여부 (메모리 절약을 위해 True로 설정)\n",
    "load_in_4bit = True\n",
    "\n",
    "# 사전 학습된 모델과 토크나이저 불러오기\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=\"Bllossom/llama-3.2-Korean-Bllossom-3B\",  # 사전 학습된 모델의 이름\n",
    "    max_seq_length=200,  # 입력 시퀀스의 최대 길이 제한 설정\n",
    "    dtype=dtype,  # 위에서 설정한 데이터 타입을 적용\n",
    "    load_in_4bit=load_in_4bit,  # 4비트 양자화를 적용하여 메모리 최적화\n",
    "    device_map=\"auto\", # 모델을 자동으로 GPU 또는 CPU에 할당하도록 설정\n",
    ")\n",
    "\n",
    "tokenizer.padding_side = \"right\"  # 오른쪽 패딩\n",
    "\n",
    "# 체크포인트 경로\n",
    "checkpoint_path = \"./outputs/checkpoint-3180\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, concatenate_datasets\n",
    "\n",
    "kmmlu_categories = [\n",
    "    \"Accounting\", \"Agricultural-Sciences\", \"Aviation-Engineering-and-Maintenance\", \"Biology\", \n",
    "    \"Chemical-Engineering\", \"Chemistry\", \"Civil-Engineering\", \"Computer-Science\", \"Construction\", \n",
    "    \"Criminal-Law\", \"Ecology\", \"Economics\", \"Education\", \"Electrical-Engineering\", \n",
    "    \"Electronics-Engineering\", \"Energy-Management\", \"Environmental-Science\", \"Fashion\", \"Food-Processing\", \n",
    "    \"Gas-Technology-and-Engineering\", \"Geomatics\", \"Health\", \"Industrial-Engineer\", \"Information-Technology\", \"Interior-Architecture-and-Design\", \n",
    "    \"Law\", \"Machine-Design-and-Manufacturing\", \"Management\", \"Maritime-Engineering\", \"Marketing\", \"Materials-Engineering\",\n",
    "    \"Mechanical-Engineering\", \"Nondestructive-Testing\", \"Patent\", \"Political-Science-and-Sociology\", \"Psychology\",\n",
    "    \"Public-Safety\", \"Railway-and-Automotive-Engineering\", \"Real-Estate\", \"Refrigerating-Machinery\", \"Social-Welfare\",\n",
    "    \"Taxation\", \"Telecommunications-and-Wireless-Technology\", \"Korean-History\", \"Math\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모든 카테고리 저장할 리스트\n",
    "eval_list = []\n",
    "train_list = []\n",
    "\n",
    "for category in kmmlu_categories:\n",
    "    # KMMLU 카테고리별 데이터셋 불러오기\n",
    "    dataset = load_dataset(\"HAERAE-HUB/KMMLU\", category, num_proc=8)\n",
    "    # train과 dev 데이터셋 각각 저장\n",
    "    train_dataset = dataset['train']\n",
    "    eval_dataset = dataset['dev']\n",
    "    \n",
    "    train_list.append(train_dataset)\n",
    "    eval_list.append(eval_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모든 데이터셋을 하나로 합치기\n",
    "train_dataset = concatenate_datasets(train_list)\n",
    "eval_dataset = concatenate_datasets(eval_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"학습 데이터: {len(train_dataset)}개\")\n",
    "print(f\"평가 데이터: {len(eval_dataset)}개\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "문제: {question}\n",
    "A. {A}\n",
    "B. {B}\n",
    "C. {C}\n",
    "D. {D}\n",
    "정답:{answer}\"\"\"\n",
    "\n",
    "EOS_TOKEN = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(texts):\n",
    "    question = texts['question']\n",
    "    A = texts['A']\n",
    "    B = texts['B']\n",
    "    C = texts['C']\n",
    "    D = texts['D']\n",
    "    answer = []\n",
    "    for idx in texts['answer']:\n",
    "        answer.append(['A', 'B', 'C', 'D'][idx-1])\n",
    "\n",
    "    result = []\n",
    "    \n",
    "    for q, a, b, c, d, ans in zip(question, A, B, C, D, answer):\n",
    "        text = prompt_template.format(question=q, A=a, B=b, C=c, D=d, answer=ans)\n",
    "        text += EOS_TOKEN\n",
    "        result.append(text)\n",
    "        \n",
    "    return {\n",
    "        \"text\": result\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전처리 적용\n",
    "train_dataset = train_dataset.map(\n",
    "    preprocess_function, \n",
    "    batched=True, \n",
    "    num_proc=4,\n",
    "    remove_columns=train_dataset.column_names\n",
    ")\n",
    "\n",
    "eval_dataset = eval_dataset.map(\n",
    "    preprocess_function, \n",
    "    batched=True, \n",
    "    num_proc=4,\n",
    "    remove_columns=eval_dataset.column_names\n",
    ")\n",
    "\n",
    "# 총 스텝 수 계산\n",
    "total_steps = len(train_dataset) // (4 * 4)  # batch_size * gradient_accumulation_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 초기화\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=8,  # 0보다 큰 어떤 숫자도 선택 가능! 8, 16, 32, 64, 128이 권장됩니다.\n",
    "    lora_alpha=16,  # LoRA 알파 값을 설정합니다. # 튜토리얼은 16\n",
    "    lora_dropout=0.01,  # 드롭아웃을 지원합니다. # # Supports any, but = 0 is optimized\n",
    "    target_modules=[\n",
    "        \"q_proj\", # query\n",
    "        \"k_proj\", # key\n",
    "        \"v_proj\", # value\n",
    "        \"o_proj\", # output (어텐션 최종 출력)\n",
    "        \"gate_proj\", # 게이트 조절 시 사용\n",
    "        \"up_proj\", # 차원 확장 시 사용\n",
    "        \"down_proj\", # 차원 축소 시 사용\n",
    "    ],  # 타겟 모듈을 지정합니다.\n",
    "    bias=\"none\",  # 바이어스를 지원합니다.\n",
    "    # True 또는 \"unsloth\"를 사용하여 매우 긴 컨텍스트에 대해 VRAM을 30% 덜 사용하고, 2배 더 큰 배치 크기를 지원합니다.\n",
    "    use_gradient_checkpointing=\"unsloth\",\n",
    "    random_state=3407,  # 난수 상태를 설정합니다. # 공식 : 3407\n",
    "    use_rslora=True,  # 순위 안정화 LoRA를 지원합니다.\n",
    "    loftq_config=None,  # LoftQ를 지원합니다.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    # 배치 설정\n",
    "    per_device_train_batch_size=4,       # 훈련 시 배치 사이즈\n",
    "    per_device_eval_batch_size=4,        # 평가 시 배치 사이즈\n",
    "    gradient_accumulation_steps=4,       # gradient 누적 단계 수 (메모리 최적화)\n",
    "\n",
    "    # 웜업 및 에폭\n",
    "    warmup_ratio=0.1,                    # 전체 스텝 대비 warmup 비율\n",
    "    num_train_epochs=1,                  # 총 학습 에폭 수\n",
    "\n",
    "    # 최적화 및 학습률 스케줄링\n",
    "    learning_rate=2e-4,                  # 초기 학습률\n",
    "    optim=\"adamw_bnb_8bit\",              # 8비트 AdamW 최적화\n",
    "    weight_decay=0.01,                   # 가중치 감소율\n",
    "    lr_scheduler_type=\"cosine\",          # 코사인 학습률 스케줄러\n",
    "\n",
    "    # 하드웨어 호환이 필요한 최적화\n",
    "    fp16=not torch.cuda.is_bf16_supported(),  # FP16 사용 (BF16 미지원 시)\n",
    "    bf16=torch.cuda.is_bf16_supported(),      # BF16 사용 (지원 시)\n",
    "    torch_compile=True,                       # PyTorch 컴파일을 통한 메모리, 학습 최적화\n",
    "\n",
    "    # 로깅 및 wandb\n",
    "    logging_steps=total_steps // 20,       # 전체 스텝의 5%마다 로깅\n",
    "    report_to=\"wandb\",                    # wandb\n",
    "    run_name=\"KMMLU_ALL_CATEGORIES\",      # run 이름\n",
    "\n",
    "    # 평가 및 체크포인트\n",
    "    eval_strategy=\"steps\",                # 정기적인 평가\n",
    "    eval_steps=total_steps // 20,         # 전체 스텝의 5%마다 평가\n",
    "    load_best_model_at_end=True,          # 최고 성능 모델 저장\n",
    "    metric_for_best_model=\"eval_loss\",         # 성능 평가 기준 (loss)\n",
    "    greater_is_better=False,              # 낮은 loss가 더 좋은 모델로 평가\n",
    "\n",
    "    # 체크포인트 저장\n",
    "    save_strategy=\"steps\",                # 체크포인트 저장 전략 (스텝 단위)\n",
    "    save_steps=total_steps // 20,         # 전체 스텝의 5%마다 저장\n",
    "    save_total_limit=5,                   # 최대 체크포인트 저장 개수\n",
    "\n",
    "    # 기타 설정\n",
    "    seed=3407,                            # 시드 고정\n",
    "    output_dir=\"outputs\",                 # 출력 경로\n",
    "    max_grad_norm=1.0,                    # 최대 gradient norm 설정\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    dataset_text_field=\"text\",\n",
    "    dataset_num_proc=4,\n",
    "    packing=False,\n",
    "    args=training_args,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "from huggingface_hub import HfApi\n",
    "from peft import PeftModel\n",
    "import os\n",
    "import json\n",
    "\n",
    "wandb.login()\n",
    "\n",
    "# WandB 설정\n",
    "wandb.init(\n",
    "    project=\"kmmlu-finetuning\",  # 프로젝트명 \n",
    "    config={\n",
    "        \"model_name\": \"Bllossom/llama-3.2-Korean-Bllossom-3B\",\n",
    "        \"dataset\": \"HAERAE-HUB/KMMLU\",\n",
    "        \"categories\": kmmlu_categories,  # 사용된 카테고리 목록\n",
    "        \"train_samples\": len(train_dataset),\n",
    "        \"eval_samples\": len(eval_dataset),\n",
    "        \"batch_size\": training_args.per_device_train_batch_size,\n",
    "        \"learning_rate\": training_args.learning_rate,\n",
    "        \"epochs\": training_args.num_train_epochs,\n",
    "        \"warmup_ratio\": training_args.warmup_ratio,\n",
    "        \"weight_decay\": training_args.weight_decay,\n",
    "        \"lora_r\": 8,  # LoRA rank\n",
    "        \"lora_alpha\": 16,  # LoRA alpha\n",
    "        \"lora_dropout\": 0.01,\n",
    "        \"max_seq_length\": 200,\n",
    "    },\n",
    "    tags=[\"kmmlu\", \"llama\", \"lora\"]\n",
    ")\n",
    "\n",
    "trainer_stats = trainer.train(resume_from_checkpoint=checkpoint_path)\n",
    "\n",
    "# 학습 완료 후 모델과 어댑터 결합 및 업로드\n",
    "def merge_and_upload_model(base_model, adapter_path, repo_id):\n",
    "    \"\"\"\n",
    "    학습된 어댑터를 베이스 모델과 결합하고 HuggingFace Hub에 업로드\n",
    "    \n",
    "    Args:\n",
    "        base_model: 기본 모델\n",
    "        adapter_path: 학습된 어댑터 경로\n",
    "        repo_id: 업로드할 HuggingFace 레포지토리 ID (예: \"username/model-name\")\n",
    "    \"\"\"\n",
    "    # 임시 저장 경로\n",
    "    merged_model_path = \"./merged_model\"\n",
    "    \n",
    "    # 모델과 어댑터 결합\n",
    "    print(\"Merging model with adapter...\")\n",
    "    merged_model = base_model.merge_and_unload()\n",
    "    \n",
    "    # 결합된 모델 저장\n",
    "    print(f\"Saving merged model to {merged_model_path}...\")\n",
    "    merged_model.save_pretrained(merged_model_path, safe_serialization=True)\n",
    "    tokenizer.save_pretrained(merged_model_path)\n",
    "    \n",
    "    # config 파일 저장\n",
    "    config_path = os.path.join(merged_model_path, \"config.json\")\n",
    "    with open(config_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump({\n",
    "            \"base_model\": \"Bllossom/llama-3.2-Korean-Bllossom-3B\",\n",
    "            \"training_dataset\": \"HAERAE-HUB/KMMLU\",\n",
    "            \"categories\": kmmlu_categories,\n",
    "            \"max_seq_length\": 200,\n",
    "            \"prompt_template\": prompt_template,\n",
    "        }, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    # HuggingFace Hub에 업로드\n",
    "    print(f\"Uploading merged model to {repo_id}...\")\n",
    "    api = HfApi()\n",
    "    api.create_repo(repo_id, exist_ok=True)\n",
    "    api.upload_folder(\n",
    "        folder_path=merged_model_path,\n",
    "        repo_id=repo_id,\n",
    "        commit_message=\"Upload merged model with KMMLU training\"\n",
    "    )\n",
    "    \n",
    "    print(\"Model successfully uploaded to HuggingFace Hub!\")\n",
    "        \n",
    "    finally:\n",
    "        # 임시 파일 정리\n",
    "        if os.path.exists(merged_model_path):\n",
    "            import shutil\n",
    "            shutil.rmtree(merged_model_path)\n",
    "            \n",
    "# 학습 완료 후 사용 예시:\n",
    "repo_id = \"Yeongi/llama-kmmlu-PEFT-finetuned\"  # 본인의 허깅페이스 레포지토리 ID로 변경\n",
    "merge_and_upload_model(model, checkpoint_path, repo_id)\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU 메모리 초기화\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kmmlu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
